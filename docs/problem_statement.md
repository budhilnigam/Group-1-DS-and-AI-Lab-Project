# **RAG-Based LLM Code Review Agent**

# **Problem Statement**

Manual review of small pull request (PR) diffs is a standard practice in collaborative Python software development workflows to enforce coding style and best-practice guidelines. Existing static analysis tools such as linters detect rule-based violations but lack the ability to generate explanatory feedback, while general-purpose Large Language Models (LLMs) can provide suggestions but often produce comments that are not aligned with project-specific coding standards.

This project aims to investigate whether incorporating retrieved project-specific knowledge improves the correctness and usefulness of automated code review comments generated by an LLM. The proposed system will analyze single-file Python pull request diffs containing up to 200 modified lines and generate review comments ***limited to localized guideline violations, including style and common Python best practices, while excluding functional correctness, security, and architectural issues.***

Issues considered in this study will be limited to the following guideline violation categories: indentation inconsistencies, naming convention violations, unused imports, mutable default arguments, and documentation or formatting-related deviations from Python style guidelines. **These categories will constitute the complete evaluation set for issue detection.** 

A Retrieval-Augmented Generation (RAG) framework will be used to retrieve relevant coding guidelines, documentation, and previously accepted pull request review discussions from the project repository at the diff-chunk level. Generated comments will be considered grounded if they explicitly reference a retrieved guideline, rule, or project-specific documentation relevant to the modified code region.

Publicly available code review datasets will be partitioned into separate training and evaluation repositories to prevent memorization effects. Evaluation pull requests will not be indexed in the retrieval database used during inference.

The performance of the proposed system will be evaluated by comparing its review comments with those generated by (i) a baseline LLM without retrieval and (ii) static analysis tools. Evaluation will primarily measure issue detection accuracy and grounding rate. Outputs from static analysis tools will be mapped to the predefined guideline violation categories prior to comparison with generated review comments from the LLM-based systems. Issue detection accuracy will be computed at the violation-instance level by comparing whether the generated review identifies the same category of guideline violation as noted in the corresponding human review comment for the pull request diff. A hallucinated comment will be defined as a generated review suggestion that references a guideline violation not applicable to the given code diff or unsupported by retrieved project knowledge. Additional analysis will include semantic similarity with human review comments, where semantic similarity exceeding a predefined BERTScore F1 threshold will be treated as supporting evidence of alignment with reviewer intent.

**Human-written review comments associated with accepted pull request changes will be treated as ground truth for guideline violations.** In cases where multiple human review comments are associated with a single pull request, each accepted comment corresponding to a guideline violation will be treated as a separate ground truth instance during evaluation. Usability of the generated comments will be assessed through precision of identified issues and simulated suggestion acceptance rate based on alignment with these accepted human-reviewed changes. Latency overhead introduced by retrieval will also be recorded as a practical deployment metric.

***The proposed tool is intended to assist human reviewers in identifying guideline violations and does not aim to replace manual code review processes.***