#          **RAG-Based LLM Code Review Agent**

                                       Data Science and AI Lab Project

                                  **Milestone1 Report**  
                                          Group 1

| Name | Email |
| :---- | :---- |
| Jeevika S | 21f3001259@ds.study.iitm.ac.in |
| Budhil Nigam | 23f1001585@ds.study.iitm.ac.in |
| Kannan S | 21f3000990@ds.study.iitm.ac.in |
| Omkar | 22f2001265@ds.study.iitm.ac.in  |
| Karunesh | 221001606@ds.study.iitm.ac.in |

**Contents**

**[1\. Abstract	3](#abstract)**

[**2\. Problem Statement	3**](#problem-statement)

[**3\. Relevant Stakeholders	4**](#relevant-stakeholders)

[**4\. Objectives	5**](#objectives)

[**5\. Literature Review	5**](#literature-review)

[5.1. Existing Approaches in Automated Code Review	6](#existing-approaches-in-automated-code-review)

[5.2. Learning-Based and LLM-Based Code Review Approaches	7](#learning-based-and-llm-based-code-review-approaches)

[5.3. Retrieval-Augmented Generation in Code Contexts	7](#retrieval-augmented-generation-in-code-contexts)

[5.4. Evaluation Practices and Metrics	8](#evaluation-practices-and-metrics)

[5.5. Comparative Analysis of Existing Approaches	8](#comparative-analysis-of-existing-approaches)

[5.6. How the Proposed Approach Differs	10](#how-the-proposed-approach-differs)

[5.7. Baseline Models for Comparison	10](#baseline-models-for-comparison)

[5.8. Identified Gaps and Research Opportunities	11](#identified-gaps-and-research-opportunities)

[**References	12**](#references)

1. # **Abstract** {#abstract}

     
   Manual pull request (PR) review is a critical but time-consuming step in Python software development, particularly for enforcing coding style and best-practice guidelines. Static analysis tools detect rule-based violations but lack explanatory feedback, while Large Language Models (LLMs) often generate ungrounded or project-misaligned review comments. This project investigates whether incorporating project-specific knowledge through a Retrieval-Augmented Generation (RAG) framework influences the accuracy and usefulness of automated code review comments in comparison to non-retrieval baselines. The proposed system analyzes single-file Python PR diffs with up to 200 modified lines and generates localized comments limited to five predefined guideline violation categories: indentation inconsistencies, naming convention violations, unused imports, mutable default arguments, and documentation or formatting deviations. Relevant guidelines and prior accepted review discussions are retrieved at the diff-chunk level to ground generated feedback. Evaluation compares the RAG-based system with a baseline LLM and static analysis tools using violation-level detection accuracy, grounding rate, semantic similarity to human reviews, and latency overhead.

2. # **Problem Statement** {#problem-statement}

   Manual review of small pull request (PR) diffs is a standard practice in collaborative Python software development workflows to enforce coding style and best-practice guidelines. Existing static analysis tools such as linters detect rule-based violations but lack the ability to generate explanatory feedback, while general-purpose Large Language Models (LLMs) can provide suggestions but often produce comments that are not aligned with project-specific coding standards.  
   This project aims to investigate whether incorporating retrieved project-specific knowledge improves the correctness and usefulness of automated code review comments generated by an LLM. The proposed system will analyze single-file Python pull request diffs containing up to 200 modified lines and generate review comments ***limited to localized guideline violations, including style and common Python best practices, while excluding functional correctness, security, and architectural issues.***  
   Issues considered in this study will be limited to the following guideline violation categories: indentation inconsistencies, naming convention violations, unused imports, mutable default arguments, and documentation or formatting-related deviations from Python style guidelines. **These categories will constitute the complete evaluation set for issue detection.**   
   A Retrieval-Augmented Generation (RAG) framework will be used to retrieve relevant coding guidelines, documentation, and previously accepted pull request review discussions from the project repository at the diff-chunk level. Generated comments will be considered grounded if they explicitly reference a retrieved guideline, rule, or project-specific documentation relevant to the modified code region.  
   Publicly available code review datasets will be partitioned into separate training and evaluation repositories to prevent memorization effects. Evaluation pull requests will not be indexed in the retrieval database used during inference.  
   The performance of the proposed system will be evaluated by comparing its review comments with those generated by (i) a baseline LLM without retrieval and (ii) static analysis tools. Evaluation will primarily measure issue detection accuracy and grounding rate. Outputs from static analysis tools will be mapped to the predefined guideline violation categories prior to comparison with generated review comments from the LLM-based systems. Issue detection accuracy will be computed at the violation-instance level by comparing whether the generated review identifies the same category of guideline violation as noted in the corresponding human review comment for the pull request diff. A hallucinated comment will be defined as a generated review suggestion that references a guideline violation not applicable to the given code diff or unsupported by retrieved project knowledge. Additional analysis will include semantic similarity with human review comments, where semantic similarity exceeding a predefined BERTScore F1 threshold will be treated as supporting evidence of alignment with reviewer intent.  
   **Human-written review comments associated with accepted pull request changes will be treated as ground truth for guideline violations.** In cases where multiple human review comments are associated with a single pull request, each accepted comment corresponding to a guideline violation will be treated as a separate ground truth instance during evaluation. Usability of the generated comments will be assessed through precision of identified issues and simulated suggestion acceptance rate based on alignment with these accepted human-reviewed changes. Latency overhead introduced by retrieval will also be recorded as a practical deployment metric.  
   ***The proposed tool is intended to assist human reviewers in identifying guideline violations and does not aim to replace manual code review processes.***

3. # **Relevant Stakeholders** {#relevant-stakeholders}

   The primary stakeholders for this system include:  
* **Software Developers** submitting pull requests who receive automated feedback.  
* **Code Reviewers** responsible for enforcing coding standards.  
* **Repository Maintainers** seeking consistency in style and documentation.  
* **Organizations and Teams** aiming to reduce review overhead while maintaining code quality.  
* **Tool Developers** building intelligent review automation systems.  
  The problem arises in collaborative development environments where large volumes of pull requests increase review workload and variability in guideline enforcement.


4. # **Objectives** {#objectives}

   

The objectives of this project are:

* To design a RAG-based LLM system capable of generating grounded review comments for single-file Python pull request diffs containing up to 200 modified lines.

* To retrieve relevant project-specific information such as coding standards, documentation, and past accepted review discussions during the review process.

* To compare review comments generated by:  
  * A baseline LLM without retrieval  
  * The proposed RAG-based system.

* To measure:  
  * Issue detection accuracy.  
  * Hallucination rate based on unsupported or inapplicable guideline references.  
  * Semantic agreement with human review comments as a supporting analysis metric.  
* To evaluate the usability of generated suggestions through issue-level precision and simulated suggestion acceptance rate.  
* To record the latency overhead introduced by the retrieval component during inference.  
* To deploy a prototype interface capable of generating automated review comments for Python pull request diffs.


5. # **Literature Review** {#literature-review}

   1. ## Existing Approaches in Automated Code Review {#existing-approaches-in-automated-code-review}

   Traditional automated code review is dominated by static analysis tools such as Pylint and Flake8, which are widely used to detect syntactic errors and violations of Python style guidelines. These tools rely on predefined, rule-based checks and are effective at consistently identifying common issues such as unused imports or formatting errors. 

   

   However, their feedback is typically limited to concise rule descriptions and does not provide contextual explanations tailored to repository-specific conventions. Large-scale empirical studies of modern code review practices ([McIntosh et al., 2016](https://dl.acm.org/doi/10.1145/2901739.2901749)) show that human review often involves contextual reasoning, historical precedent, and project-specific interpretation, aspects not captured by static analyzers.

   

   Several platforms integrate static analyzers into pull request workflows, including Sider and Qodana. While these systems improve automation and coverage, they remain fundamentally rule-driven and do not adapt to repository-specific coding standards or historical review practices.

   

   Recent research, such as [STYLE-ANALYZER (Markovtsev et al., 2019\)](https://doi.org/10.48550/arXiv.1904.00935) introduces repository-aware style analysis using unsupervised learning to detect formatting inconsistencies. Although such approaches improve adaptability to project-specific styles, they primarily focus on automated style fixing rather than generating explanatory review comments or handling pull request diffs directly. Few learning-based approaches have emerged to automate review comment generation using historical pull request data. Models such as [AUGER (Siow et al., 2022\)](https://doi.org/10.48550/arXiv.2208.08014) leverage pre-trained transformer architectures to generate natural language review comments from code diffs and associated context. While these systems produce fluent and human-like feedback, they often rely heavily on training data and may generate generic or imprecise comments due to the absence of explicit grounding in project documentation or coding guidelines.

   

   In contrast, general-purpose LLM-based coding assistants can generate natural-language review comments but typically operate without access to project-specific guidelines or documentation, leading to inconsistent or misaligned feedback. These limitations motivate the exploration of retrieval-augmented approaches that combine the precision of static analysis with contextual grounding from project knowledge.

   2. ## Learning-Based and LLM-Based Code Review Approaches {#learning-based-and-llm-based-code-review-approaches}

   To address limitations of rule-based systems, learning-based approaches have been proposed. Early work, such as [*Automatic Code Review by Learning the Revision of Source Code* (Tufano et al., 2018\)](https://arxiv.org/abs/1812.08693), models code diffs directly to learn revision patterns from historical data. These methods demonstrate that pull request diffs contain learnable structures that can support automation.

   More recently, transformer-based models have been applied to review comment generation. [*CodeReviewer: Pre-Training for Automatic Code Review* (Li et al., 2022\)](https://arxiv.org/abs/2203.09095) introduces a pre-trained transformer model specifically designed for generating review comments from code changes. Such models produce fluent and human-like feedback, demonstrating the capability of large language models in review settings.

   However, LLM-based systems face several limitations:

* Hallucinated or inapplicable comments

* Overgeneralized suggestions

* Lack of traceability to repository guidelines

* Sensitivity to prompt design

  [*Evaluations of large language models trained on code (Chen et al., 2021\)*](https://arxiv.org/abs/2107.03374) show strong generative ability but also highlight brittleness and inconsistency in reasoning over unseen contexts.

  Most learning-based review systems evaluate performance using metrics such as accuracy, precision, recall, and semantic similarity (e.g., BLEU or BERTScore), but rarely assess whether generated comments are explicitly grounded in retrievable repository knowledge.

## 

  3. ## Retrieval-Augmented Generation in Code Contexts {#retrieval-augmented-generation-in-code-contexts}

  Retrieval-Augmented Generation (RAG), introduced by Lewis et al. (2020) in [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401), combines a retrieval module with a generative model to improve factual grounding in knowledge-intensive tasks. Instead of relying solely on parametric knowledge, the model conditions generation on retrieved contextual information.

  RAG has demonstrated improvements in question answering and domain-specific reasoning tasks. In the context of software engineering, empirical comparisons such as [*An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection* (2026)](https://arxiv.org/pdf/2601.00254) show that retrieval mechanisms can improve precision in code-related tasks by grounding outputs in relevant contextual artifacts.

  Benchmarking studies of open-source LLMs in RAG pipelines for code tasks further emphasize the importance of retrieval quality in influencing downstream generation performance ([Springer, 2025](https://link.springer.com/article/10.1007/s44427-025-00006-3)).

  However, existing RAG research in code primarily focuses on:

* Vulnerability detection  
* Code question answering  
* Code summarization  
  There is limited empirical investigation of RAG applied specifically to diff-level, style-constrained pull request review workflows.


  4. ## Evaluation Practices and Metrics {#evaluation-practices-and-metrics}

  Across automated code review and LLM-based code tasks, common evaluation metrics include:

* Precision, Recall, and F1 score  
* Accuracy  
* [BLEU (Papineni et al., 2002\)](https://aclanthology.org/P02-1040/)  
* [BERTScore (Zhang et al., 2019\)](https://arxiv.org/abs/1904.09675)  
* Human evaluation for usefulness  
  Static tools optimize rule-detection accuracy and consistency. LLM-based systems often report semantic similarity to human-written comments as a proxy for quality.  
  However, explicit measurement of grounding, whether generated comments are traceable to repository-specific guidelines or prior accepted review discussions, is rarely treated as a primary evaluation dimension in automated review research.

  5. ## Comparative Analysis of Existing Approaches {#comparative-analysis-of-existing-approaches}

1. **Static Analysis Tools**

   **Strengths**

* Deterministic rule enforcement  
* High precision for predefined checks  
* Low computational cost  
* Mature CI/CD integration

  **Weaknesses**

* Limited to predefined rules  
* No contextual explanation  
* No adaptation to repository-specific review history  
* No grounding beyond rule references

2. **Learning-Based Review Systems**

   **Strengths**

* Can model diff patterns from historical data  
* Capable of generating natural-language comments  
* Adaptable to repository patterns

  **Weaknesses**

* Depend heavily on training data quality  
* May generate generic suggestions  
* Limited traceability to specific guidelines  
* Grounding rarely measured explicitly

3.  General LLM-Based Approaches

   **Strengths**

* Fluent and human-like feedback  
* Strong reasoning over code context  
* Flexible and adaptable to new repositories

  **Weaknesses**

* Hallucination risk  
* Inconsistent adherence to repository guidelines  
* Sensitive to prompt design  
* Lack of explicit grounding mechanisms

  6. ## How the Proposed Approach Differs {#how-the-proposed-approach-differs}

  To clearly contrast the proposed system with existing approaches, the following structured comparison is provided:

| Approach | Rule-Based Precision | Natural Language Explanation | Repository-Aware Adaptation | Explicit Grounding Mechanism | Diff-Level Context Handling |
| ----- | ----- | ----- | ----- | ----- | ----- |
| Static Analysis Tools | High | Limited | No | Limited (rule references only) | Partial |
| Learning-Based Review Systems | Moderate | Yes | Partial | Rarely Explicit | Yes |
| General LLM-Based Systems | Moderate | Yes | No | No | Yes |
| **Proposed RAG-Based System** | High | Yes | Yes | Yes | Yes |

## 

  The proposed system differs by explicitly retrieving repository-specific coding guidelines, documentation, and prior accepted review discussions during inference. Unlike static tools that rely solely on predefined rules, and unlike LLM-only systems that depend entirely on parametric knowledge, the RAG-based approach conditions generation on dynamically retrieved project context.

  Additionally, grounding is treated as a measurable evaluation criterion rather than an implicit property. Generated comments are considered grounded only when they explicitly reference relevant retrieved guidelines or prior accepted review discussions corresponding to the modified code region.

     

  7. ## Baseline Models for Comparison {#baseline-models-for-comparison}

  The study will compare the proposed RAG-based system against two clearly defined baselines:

1. **Static Analysis Baseline**  
    Rule-based detection using tools such as Pylint and Flake8. Outputs from these tools will be mapped to the predefined guideline violation categories used in this study.

2. **LLM Baseline (No Retrieval)**  
    A large language model prompted directly with the pull request diff, without access to repository-specific guidelines or retrieved contextual information.

3. **Proposed RAG-Based System**  
    A large language model augmented with retrieval of project-specific guidelines and prior accepted review discussions at the diff-chunk level.

   This structured comparison ensures that improvements, if observed, can be attributed specifically to the inclusion of retrieval and grounding mechanisms rather than general LLM capability.

   8. ## Identified Gaps and Research Opportunities {#identified-gaps-and-research-opportunities}

   From the reviewed literature, several gaps emerge:

1. Most RAG-based research in code focuses on vulnerability detection or QA tasks rather than structured style-based pull request review.  
2. Learning-based review models generate fluent comments but lack explicit repository-aware grounding mechanisms.  
3. Evaluation metrics emphasize similarity and detection accuracy but rarely quantify grounding or traceability.  
4. Diff-level retrieval for localized review comment generation remains underexplored.  
   These gaps create an opportunity to empirically evaluate whether retrieval improves grounding, alignment, and usability in automated style-based pull request review.  
   These gaps create the following research opportunity:  
* To evaluate whether retrieval improves grounding and traceability of generated review comments.  
* To assess whether explicit repository-aware context improves alignment with human reviewer intent.  
* To introduce grounding rate as a primary evaluation metric in automated style-based pull request review.  
* To empirically compare retrieval-augmented generation against both deterministic rule-based tools and non-retrieval LLM systems in a constrained, realistic review setting.  
  The contribution of this work lies not in proposing a new foundational language model, but in systematically evaluating whether retrieval enhances correctness, contextual alignment, and usability in automated style-constrained pull request review workflows.


  # **References** {#references}

* McIntosh, S., et al. (2016). *A Large-Scale Study of Modern Code Review.* ACM MSR.

* Markovtsev, V., et al. (2019). *Style-Analyzer: Fixing Code Style Inconsistencies with Unsupervised Learning.* arXiv:1908.02737.

* Siow, J., et al. (2022). *AUGER: Automated Code Review Comment Generation.* arXiv:2208.08014

* Tufano, M., et al. (2018). *Automatic Code Review by Learning the Revision of Source Code.* arXiv:1812.08693.

* Li, Z., et al. (2022). *CodeReviewer: Pre-Training for Automatic Code Review.* arXiv:2203.09095.

* Chen, M., et al. (2021). *Evaluating Large Language Models Trained on Code.* arXiv:2107.03374.

* Lewis, P., et al. (2020). *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.* arXiv:2005.11401.

* (2026). *An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection.* arXiv:2601.00254.

* (2025). *Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Code Tasks.* Springer.

* Papineni, K., et al. (2002). *BLEU: A Method for Automatic Evaluation of Machine Translation.* ACL.

* Zhang, T., et al. (2019). *BERTScore: Evaluating Text Generation with BERT.* arXiv:1904.09675.